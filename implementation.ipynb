{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMsurrIOU1iXzCaSFWL+nrM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JAT6hNart9bX"},"outputs":[],"source":["#molecular_generation.py file incelemesidir.\n","import json\n","from argparse import ArgumentParser\n","from transformers import RobertaTokenizer, RobertaForCausalLM\n","from pathlib import Path\n","\n","\n","def main(args):\n","    ckpt = Path(args.model)\n","    # Assigning a path to variable ckpt\n","    tokenizer = RobertaTokenizer.from_pretrained(str(ckpt))\n","    # We can load and save a tokenizer through from_pretrained() and save_pretrained() methods.\n","    model = RobertaForCausalLM.from_pretrained(str(ckpt))\n","    # Loadining pre-trained model through from_pretrained method\n","    # The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach. It is based on Google’s BERT model released in 2018.\n","    # It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training \n","    # with much larger mini-batches and learning rates.\n","    input_ids = tokenizer.encode('', return_tensors='pt')\n","    # Return_tensors = “pt” is just for the tokenizer to return PyTorch tensors. Encoding defines input ids for tokens\n","    args_dict = vars(args)\n","    # Make args a dictionary.\n","    generation_params = {k:v for k, v in args_dict.items() if k not in ['model', 'output_file']}\n","    output = model.generate(input_ids, **generation_params)\n","    # Generating outputs from input ids and model\n","    output_decoded = [tokenizer.decode(item, skip_special_tokens=True) for item in output]\n","    # Convert id to decoded version using tokenizer.\n","    with open(f'predictions/{args.model}.json', 'w') as f:\n","        f.write(json.dumps({'predictions': output_decoded, 'model': str(ckpt), **generation_params}))\n","        # Write predictions to .json file\n","    \n","\n","if __name__ == \"__main__\":\n","    parser = ArgumentParser()\n","    parser.add_argument('--model', type=str, required=True)\n","    parser.add_argument('--num_return_sequences', type=int, required=True)\n","    # num_return_sequences : The number of independently computed returned sequences for each element in the batch.\n","    parser.add_argument('--do_sample', action='store_true')\n","    # Whether or not to use sampling ; use greedy decoding otherwise.\n","    parser.add_argument('--top_k', type=int, default=0)\n","    # The number of highest probability vocabulary tokens to keep for top-k-filtering.\n","    parser.add_argument('--max_length', type=int, default=None)\n","    # The maximum length the generated tokens can have\n","    parser.add_argument('--top_p', type=float)\n","    # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n","    args = parser.parse_args()\n","    main(args) \n","\n","\n","    #-----------------------------------------------------------------------------------------------#\n","\n"]},{"cell_type":"code","source":["import moses\n","import json\n","import pandas as pd\n","import logging\n","from pathlib import Path\n","from argparse import ArgumentParser\n","\n","\n","def read_json(filename):\n","    return json.loads(open(filename, 'r').read())\n","\n","\n","def dump_json(data, filename):\n","    json.dump(data, open(filename, 'w'))\n","\n","\n","def evaluate_model_generations(folder, train, targeted_interactions):\n","    output_folder = Path(folder)\n","    outputs = read_json(output_folder / 'ChemBERTaLM.json')\n","    moses_train = moses.get_dataset('train').tolist()\n","    # Transform moses data set to list\n","    results = {}\n","    scores_dir = Path('results') / output_folder.parent.name / output_folder.name\n","    scores_dir.mkdir(parents=True, exist_ok=True)\n","    results['metrics_moses'] = moses.get_all_metrics(sum(outputs['predictions'].values(),[]), train=(train+moses_train))\n","    #Moses is a benchmarking data set refined from ZINC Database\n","\n","    results['targets'] = {}\n","    for uniprot_id, smiles in outputs['predictions'].items():\n","        logging.info('Targeting %s' % uniprot_id)\n","        try:\n","            results['targets'][uniprot_id] = {\n","                'bdb': moses.get_all_metrics(smiles, k=1, train=(train+moses_train), test=targeted_interactions[uniprot_id]),\n","                'moses': moses.get_all_metrics(smiles, k=1)\n","            }\n","        except:\n","            results['error'].append(uniprot_id)\n","    dump_json(results, output_folder / 'ChemBERTaLM_scores.json')\n","\n","\n","parser = ArgumentParser()\n","parser.add_argument('--model', type=str, required=True)\n","parser.add_argument('--train', type=str, default='data/splits/train_interactions.csv')\n","parser.add_argument('--test', type=str, default='data/splits/test_interactions.csv')\n","args = parser.parse_args()\n","train = pd.read_csv(args.train)['canonical_SMILES'].values.tolist()\n","test_interactions = pd.read_csv(args.test, index_col=None)\n","targeted_interactions = test_interactions.groupby('UniProt_S_ID')['canonical_SMILES'].apply(list).to_dict()\n","evaluate_model_generations('predictions/' + args.model, train, targeted_interactions)"],"metadata":{"id":"rWwuqKV9uHnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#molecular_training.py dosyası yeterince commentliydi onu okumak yeterli oldu."],"metadata":{"id":"CoO-RZZQuJj1"},"execution_count":null,"outputs":[]}]}